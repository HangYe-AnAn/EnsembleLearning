% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bagging_ridge_model.R
\name{bagging_ridge_model}
\alias{bagging_ridge_model}
\title{Bagging Ridge Model}
\usage{
bagging_ridge_model(X, y, n_bags = 100, K = NULL)
}
\arguments{
\item{X}{A data frame or matrix containing the predictor variables.}

\item{y}{A vector containing the response variable (Binary or continuous).}

\item{n_bags}{Number of bootstrap samples to create (default is 100).}

\item{K}{A positive integer specifying the number of top informative predictors to select; must be greater than the number of predictors. (Optional)}
}
\value{
bagging_Linear_model returns an object of class "list". A list with:
\item{predictions}{The averaged coefficient estimates of the final fitted model.}
\item{variable_importance}{Importance of each predictor variable calculated by counting the number of times each variable appears in the non-missing coefficients across all bootstrap samples.}
}
\description{
The bagging_ridge_model function takes predictor variables (X) and a response variable (y) as inputs, along with optional parameters for the number of bags (n_bags), the number of top predictors to consider (K). It then fits multiple ridge regression models using bootstrap sampling and aggregates their predictions to improve model accuracy.
}
\details{
The function generates n_bags bootstrap samples from the original dataset.
\cr
\cr
\enumerate{For each bootstrap sample:
\item it fits a ridge regression model (using \code{\link[glmnet]{glmnet}}) using cross-validation to find the optimal regularization parameter, or minimum lambda. It then extracts the coefficients of the fitted ridge models.
\item If the K is specified, and if p >> n, it will pre-screen for top K most “informative” predictors (For more information see \code{\link[simpleEnsembleGroup7]{returnTopK}}). Then it fits a ridge regression model (using \code{\link[glmnet]{glmnet}}) using the top K predictors (For more information, see \code{\link{returnTopK}}) and using cross-validation to find the optimal regularization parameter, or minimum lambda. It then extracts the coefficients of the fitted ridge models.
}
\cr
The coefficients from all bootstrap samples are combined and aggregated by taking the average, or row means, across all bootstrap samples with equal weights. This provides the final coefficient estimates of the final ridge model.
\cr
\cr
The function calculates the importance of each predictor variable by counting the number of times each variable appears in the non-missing coefficients across all bootstrap samples.
\cr
\cr
\eqn{\alpha=0} corresponds to the Ridge penalty.
}
\examples{
# Load required libraries
library(glmnet)
# Generate sample data
n <- 100  # Number of observations
n_predictors <- 10  # Number of predictors
# Create predictor variables
predictors <- matrix(rnorm(n * n_predictors), ncol = n_predictors)

# Create names for the predictors
predictor_names <- c("Predictor1", "Predictor2", "Predictor3", "Predictor4", "Predictor5", "Predictor6", "Predictor7", "Predictor8", "Predictor9", "Predictor10")
# Assign names to the columns of the predictors matrix
colnames(predictors) <- predictor_names

# Generate response variable
response <- rnorm(n)

# Run bagging Elastic Net regression
result <- bagging_ridge_model(predictors, response, K = 5)
# Print the average coefficient estimate
print(result$predictions)
# Print variable importance
print(result$variable_importance)
}
