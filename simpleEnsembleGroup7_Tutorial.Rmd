---
title: "simpleEnsembleGroup7_Tutorial"
author: "Hang Ye, Minghao Zhang, Lily Yuan, Jiang Yi"
date: "2024-05-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Contents
1. [Installation]
2. [Introduction]
+ [Step 1.1: Handle missing values]
+ [Step 1.2: Converting the response variables.]
+ [Step 2.1: Checks the validity of K]
+ [Step 2.2: Checks the validity of method]
+ [Step 2.3: Checks the validity of n_bag]
+ [Step 2.4: Checks the validity of alpha]
+ [Step 2.5: Checks the validity of ensemble and bagging]
3. [Step 3: Quick Start on Model fitting](#quickstart)
+ [Step 3.1: Model fitting with basic model]
+ [Step 3.2: Model fitting with model with top K informative predictors]
+ [Step 3.3: Model fitting with model with bagging]
+ [Step 3.4: Model fitting with model with bagging and top K informative predictors]
+ [Step 3.5: Model fitting with model with ensemble.]
+ [Step 3.6: Model fitting with model with ensemble and top K informative predictors]
4. [Bagging Method]
5. [Ensemble Method]
6. [Top K Informative Predictors Method]


## PLEASE INSTALL THE FOLLOWING PACKAGES BEFORE USING OUR PACKAGE
```{r eval=FALSE}
install.packages("glmnet")
install.packages("e1071")

library(e1071)
library(glmnet)
```



## Installation

### Step 1: Prepare the Package:
  Ensure the package is in a format that R can install (folder ready, tar.gz for source packages, .zip for Windows binary packages, .tgz for macOS binary packages).
Optionally, compress the package folder into the required format.

### Step 2: Locate the Package Folder:
  Determine the path to the package folder on your computer.

### Step 3: Install the Package:
  Open R or RStudio.
Use the install.packages() function with the repos = NULL argument and the correct type argument for the package format.

Example for folder ready:
```{r eval=FALSE}
install.packages("/path/to/package", repos = NULL, type = "source")  
```

Example for Source Package:  
```{r eval=FALSE}
install.packages("/path/to/package.tar.gz", repos = NULL, type = "source")
```

Example for Windows Binary Package:
```{r eval=FALSE}
install.packages("/path/to/package.zip", repos = NULL, type = "win.binary")
```

Example for macOS Binary Package:
```{r eval=FALSE}
install.packages("/path/to/package.tgz", repos = NULL, type = "mac.binary")
```

For every "/path/to/package.":
+ [/path/to] is the whole path name where the folder is on your device.\
  
+ For Windows, using File Explorer:\
      1. Navigate to the folder containing your package.\
      2. Click on the address bar to display the full path.\
      3. Right-click on the path and choose "Copy address as text."\
      4. You can now paste the full path (Ctrl+V) where needed.\
      
+ For macOS, using Finder:\
      1. Open Finder and navigate to the folder containing your package.\
      2. Right-click on the folder and hold the Option key. \
      3. You should see the "Copy [FolderName] as Pathname" option.\
      4. Click on it to copy the full path.\

+ [/package] is our package names.\
    Here is /simpleEnsembleGroup7. (do not forget"/")\
    
+ After [.] is the format of different compress method. \
  For most cases, "zip" is the common compress format for Windows; "tgz" or "zip" is the common compress format for macOS.\
  If you already have the folder like other folders on your device, just ignore the [.].\


### Step 4: Verify Installation:
Load the installed package using the library() function.
Verify that the package loads without errors.

```{r}
library(simpleEnsembleGroup7)
```

## Introduction
The function simpleEnsembleGroup7 implements various regression models and it is able to apply different tasks, including bagging, pre-screen for top informative predictors, and ensemble learning.For quick start,click [here](#quickstart)

The workflow of simpleEnsemnbleGroup7 function:

1. Check and handle missing values and, if response is binary and not numeric, it converts categorical variables of the response (y) to numeric binary(0 or 1).

2. It then checks the validity of the parameters, including the number of top predictors (K), if the selected models are allowed (we only allow "linear", "ridge", "lasso", "elastic", and "svm"), number of bag samples (n_bag), alpha values (applies only if you are using elastic net method), and whether ensemble or bagging is TRUE or FALSE. 

3. Based on these checks, the function fits a model using the specified method, either with or without bagging or ensemble learning and if pre-screening for top informative predictors is applied.


4. Finally, the function returns the final fitted model or models based on the methods and parameter that the user enters.

Here we will illustrate and explain each of these steps respectively.

## Step 1.1: Handle missing values
This step will only apply when you have missing values in the predictor variables (X) and the response variable (y). Depending on the user's choice, it offers methods to handle missing values such as NA omit removal, mean imputation, median imputation, mode imputation, or retaining NA values.

The 'handle_missing_values' function provides several options for handling missing values, including:

1. Removing rows with missing values ('na.omit')
Most common method to handle missing values.
Remove all the lines with NA values.
```{r}
X <- matrix(c(1, 2, NA, 4, 5, NA, 2, 3, 4, 5), ncol = 2)
y <- c(1, NA, 3, 4, 5)
#handle_missing_values(X, y)
#In the console
#Enter your choice (1, 2, 3, 4, or 5): 1
#Missing values have been removed using na.omit.
#$X
#     [,1] [,2]
#[1,]    4    4
#[2,]    5    5
#
#$y
#[1] 4 5
```

2. Imputing missing values with the mean
This method will use the left data to calculate a mean and impute into the NA values.
```{r}
X <- matrix(c(1, 2, NA, 4, 5, NA, 2, 3, 4, 5), ncol = 2)
y <- c(1, NA, 3, 4, 5)
#handle_missing_values(X, y)
#In the console
#Enter your choice (1, 2, 3, 4, or 5): 2
```

3. Imputing missing values with the median
This method will us the left data to calculate a median and impute into the NA values.
```{r}
X <- matrix(c(1, 2, NA, 4, 5, NA, 2, 3, 4, 5), ncol = 2)
y <- c(1, NA, 3, 4, 5)
#handle_missing_values(X, y)
#In the console
#Enter your choice (1, 2, 3, 4, or 5): 3
```

4. Imputing missing values with the mode
This method will use the left data to calculate a mode and impute into the NA values.
```{r}
X <- matrix(c(1, 2, NA, 4, 5, NA, 2, 3, 4, 5), ncol = 2)
y <- c(1, NA, 3, 4, 5)
#handle_missing_values(X, y)
#In the console
#Enter your choice (1, 2, 3, 4, or 5): 4
```

5. Keeping NA values in the dataset for the next step calculation
This method keeps NA values in the dataset which will affect the model accuracy in the next steps.
*We don't recommend this option
```{r}
X <- matrix(c(1, 2, NA, 4, 5, NA, 2, 3, 4, 5), ncol = 2)
y <- c(1, NA, 3, 4, 5)
#handle_missing_values(X, y)
#In the console
#Enter your choice (1, 2, 3, 4, or 5): 5
```

### Step 1.2: Converting the response variables.
This step will only apply if you binary responses variable is not numeric. If your binary variable contains only "A" and "B", we will convert them into 1 and 0.


### Step 2.1: Checks the validity of K
We will check the validity of K using check_topK(X, y, K)

This function checks if the input of K is NULL or not. Default for K is NULL.
If K is NULL, the function returns the original X and y without further processing.

If K is not NULL, the function checks the following conditions before proceeding:

1. P > N: The number of predictors (P) must be less than the number of observations (N).

2. K is numeric: K must be a numeric value.

3. K is greater than zero: K must be greater than zero.

4. K does not exceed the number of predictors: K must not exceed the total number of predictors in X.

If all conditions are met, function will move to the next step.

### Step 2.2: Checks the validity of method
Valid options are "linear", "ridge", "lasso", "elastic", and "svm". Default method is NULL.

1. This parameter must be a single model type when bagging is TRUE. Ex. method = "linear"

2. This parameter can be multiple model types when ensemble is TRUE. A list of character strings specifying the model types to be included in ensemble learning. Ex. method = c("linear", "lasso")

### Step 2.3: Checks the validity of n_bag
The number of bagging iterations to perform. Default is 100.
1. Need to be a positive integer that is greater than 0.

### Step 2.4: Checks the validity of alpha
The alpha parameter for elastic net models, influencing the mix of ridge and lasso regularization. Default is 0.5.
1. Need be to an integer from 0 to 1, inclusive.

### Step 2.5: Checks the validity of ensemble and bagging
bagging: A logical value indicating whether bagging should be used. If bagging is TRUE, ensemble must be FALSE. Default is FALSE.

ensemble: A logical value indicating whether ensemble should be used. If ensemble is TRUE, bagging must be FALSE. Default is FALSE.

*Bagging and ensemble cannot be both TRUE.

*If the parameters is correctly entered, the function will proceeds to the next stage of the modeling process.

### Step 3: Quick Start on Model Fitting <a id="quickstart"></a>

### Step 3.1: Model fitting with basic model
Basic model fitting does not include K, bagging, ensemble, alpha, n_bag. It will return a basic model based on the single method that the user inputs.

Here is an example of fitting a basic linear model
```{r}
# Generate sample data
n <- 100  # Number of observations
n_predictors <- 10  # Number of predictors
# Create predictor variables
predictors <- matrix(rnorm(n * n_predictors), ncol = n_predictors)

# Create names for the predictors
predictor_names <- c("Predictor1", "Predictor2", "Predictor3", "Predictor4", "Predictor5", "Predictor6", "Predictor7", "Predictor8", "Predictor9", "Predictor10")
# Assign names to the columns of the predictors matrix
colnames(predictors) <- predictor_names

# Generate response variable
response <- rnorm(n)
simpleEnsembleGroup7(predictors, response, method = "linear", K = NULL, bagging = FALSE, ensemble = FALSE)
```

### Step 3.2: Model fitting with model with top K informative predictors
When the user specify a K value and no bagging or ensemble, and the number of predictors is greater than the number of observations, our function will return the model with top K informative predictors based on the single method that the user inputs.

Here is an example of fitting a ridge model with Top 5 informative predictors.
```{r}
# Generate sample data
n <- 100  # Number of observations
n_predictors <- 105  # Number of predictors
# Create predictor variables
predictors <- matrix(rnorm(n * n_predictors), ncol = n_predictors)

# Create names for the predictors
predictor_names <- paste0("Predictor", 1:105)
# Assign names to the columns of the predictors matrix
colnames(predictors) <- predictor_names

# Generate response variable
response <- rnorm(n)
result <- simpleEnsembleGroup7(predictors, response, method = "ridge", K = 5)
coef(result)
```

### Step 3.3: Model fitting with model with bagging
When user specify bagging = TRUE, our function will compute bagging and return the appropriate final averaged model based on the method chosen.

Here is an example of fitting a ridge model using bagging method and with 20 bootstrap samples.
```{r}
# Generate sample data
n <- 100  # Number of observations
n_predictors <- 10  # Number of predictors
# Create predictor variables
predictors <- matrix(rnorm(n * n_predictors), ncol = n_predictors)

# Create names for the predictors
predictor_names <- c("Predictor1", "Predictor2", "Predictor3", "Predictor4", "Predictor5", "Predictor6", "Predictor7", "Predictor8", "Predictor9", "Predictor10")
# Assign names to the columns of the predictors matrix
colnames(predictors) <- predictor_names

# Generate response variable
response <- rnorm(n)
result <- simpleEnsembleGroup7(predictors, response, method = "ridge", bagging = TRUE, n_bag = 20)
result
```
### Step 3.4: Model fitting with model with bagging and top K informative predictors
When user specify bagging = TRUE and K, our function will compute bagging, and for each bootstrap sample we will find the top K informative predictors, then return the appropriate final averaged model based on the method chosen.

Here is an example of fitting a ridge model using bagging method, with 20 bootstrap samples and also finding top 5 informative predictors for each 20 bootstrap samples.
```{r}
# Generate sample data
n <- 100  # Number of observations
n_predictors <- 105  # Number of predictors
# Create predictor variables
predictors <- matrix(rnorm(n * n_predictors), ncol = n_predictors)

# Create names for the predictors
predictor_names <- paste0("Predictor", 1:105)
# Assign names to the columns of the predictors matrix
colnames(predictors) <- predictor_names

# Generate response variable
response <- rnorm(n)
result <- simpleEnsembleGroup7(predictors, response, method = "ridge", K = 5, bagging = TRUE, n_bag = 20)
result
```
### Step 3.5: Model fitting with model with ensemble.
When user specify ensemble = TRUE and a method or methods, our function will compute ensemble return the appropriate final model or models based on the methods chosen.

Here is an example of fitting a ridge model and linear using ensemble method.
```{r}
# Generate sample data
n <- 100  # Number of observations
n_predictors <- 105  # Number of predictors
# Create predictor variables
predictors <- matrix(rnorm(n * n_predictors), ncol = n_predictors)

# Create names for the predictors
predictor_names <- paste0("Predictor", 1:105)
# Assign names to the columns of the predictors matrix
colnames(predictors) <- predictor_names

# Generate response variable
response <- rnorm(n)
result <- simpleEnsembleGroup7(predictors, response, method = c("ridge", "linear"), ensemble = TRUE)
result
coef(result$models$ridge)
```

### Step 3.6: Model fitting with model with ensemble and top K informative predictors
When user specify ensemble = TRUE, a method or methods, and K, our function will compute ensemble with find top K informative predictors and return the appropriate final model or models on the methods chosen.

Here is an example of fitting a ridge model and linear using ensemble method and with top 5 informative predictors.
```{r}
# Generate sample data
n <- 100  # Number of observations
n_predictors <- 105  # Number of predictors
# Create predictor variables
predictors <- matrix(rnorm(n * n_predictors), ncol = n_predictors)

# Create names for the predictors
predictor_names <- paste0("Predictor", 1:105)
# Assign names to the columns of the predictors matrix
colnames(predictors) <- predictor_names

# Generate response variable
response <- rnorm(n)
result <- simpleEnsembleGroup7(predictors, response, method = c("ridge", "linear"), K = 5, ensemble = TRUE)
result
coef(result$models$ridge)
```
## Bagging Method
Our bagging method include bagging_linear_model, bagging_ridge_model, bagging_lasso_model, bagging_elastic_model.

The function generates n_bags bootstrap samples from the original dataset.

For each bootstrap sample:

It fits a desired regression model and it then extracts the coefficients of the fitted regression models.

+ If the K is specified, and if p >> n, it will pre-screen for top K most “informative” predictors (For more information see returnTopK). Then it fits a desired regression model using the top K predictors and It then extracts the coefficients of the fitted Elastic Net models.

The coefficients from all bootstrap samples are combined and aggregated by taking the average, or row means, across all bootstrap samples with equal weights. This provides the final coefficient estimates of the desired regression model.

The function calculates the importance of each predictor variable by counting the number of times each variable appears in the non-missing coefficients across all bootstrap samples.

The function returns a list containing the average coefficient estimates (predictions) and the variable importance scores (variable_importance).

## Ensemble Method
Our ensemble method EnsembleLearning() implements an ensemble learning approach that allows users to train multiple models on the same dataset and aggregate the predictions into a single output. It supports both regression and classification tasks.

The 'EnsembleLearning' function trains each specified model using the provided data, then aggregates their predictions into a single result. The aggregation method depends on the type of task:

Classification: Uses majority voting to determine the final class.

Regression: Calculates the mean of all model predictions.

This method enhances prediction robustness by combining the unique strengths of different modeling approaches and reducing the risk of overfitting.

It returns a list containing:

+ prediction: The aggregated prediction from the ensemble. For classification, this is based on majority voting; for regression, it is the average of the predictions.

+ models: Details of all the individual models trained during the ensemble process. This allows for further inspection and analysis.

#### Advantanges of Majority Vote and Average Work

1. Majority Voting (for Classification):

+ Reduction of Variance: When predictions from multiple models are combined, the ensemble can reduce the variance part of the prediction error. Each individual model may have its own errors, but when combined through voting, random errors cancel out, reducing overall error.

+ Improved Accuracy: By allowing each model to vote for an outcome and selecting the outcome with the most votes, the ensemble leverages the "wisdom of the crowd," often leading to more accurate predictions than any single model, especially if the models are diverse.

+ Robustness to Overfitting: Models that fit very closely to a particular set of training data might not perform well on new data. Majority voting mitigates this by combining the strengths of multiple models, some of which may not have overfit the data.

2. Averaging (for Regression):

+ Smoothing Effect: Averaging predictions from different models tends to smooth out the predictions by reducing the effect of outliers or extreme values predicted by any single model. This generally leads to a more stable and reliable prediction.

+ Reduction of Error: Like majority voting, averaging can decrease the variance component of the error without increasing bias significantly, assuming the models are unbiased.
Compromise Between Models: If some models give too high or too low predictions, averaging these predictions can balance these biases, leading to a prediction that often sits close to the true value.


## Top K Informative Predictors Method
Our returnTopK() evaluates the informativeness of predictors in relation to a given response variable using appropriate statistical tests based on their data types. It returns the top K informative predictors along with the response variable for further analysis or modeling.

The function applies different statistical tests depending on the data type of the predictors and the response:

+ For continuous predictors and a continuous response, it uses the Pearson Correlation Coefficient.

+ For categorical predictors and a categorical response, it applies the Chi-square Test of Independence.

+ For categorical predictors and a continuous response, it uses the Kruskal-Wallis Test.

Note: The function handles NA values by omitting rows with NA in any of the predictors or the response during the analysis.

Our function returns a list of class 'list' containing the following components:

+ X: A dataframe containing the top K informative predictors.

+ y: The response variable as provided in the input.









<hr>



